{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running importer\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tweetbench.expirements import gather, make_table, run\n",
    "from tweetbench.notebook_loader import NotebookLoader\n",
    "from tweetbench.data import get_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from /Users/amagill/school/ds/store/a-m730/Project/source/expirements/baseline.ipynb\n",
      "importing Jupyter notebook from /Users/amagill/school/ds/store/a-m730/Project/source/expirements/001.ipynb\n"
     ]
    }
   ],
   "source": [
    "expirements = gather()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading in data...\n"
     ]
    }
   ],
   "source": [
    "splits = get_splits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=1.0, class_weight=None, dual=False, '\n",
      "               'fit_intercept=True, intercept_scaling=1, l1_ratio=None, '\n",
      "               'max_iter=100, multi_class=auto, n_jobs=None, penalty=l2, '\n",
      "               'random_state=None, solver=lbfgs, tol=0.0001, verbose=0, '\n",
      "               'warm_start=False, )',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amagill/.envs/38/fakenews/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.463s\n",
      "test time:  0.111s\n",
      "\n",
      "mcc:   0.433,   prec:   0.582,   rec:   0.528,   acc:   0.766\n",
      "\n",
      "confusion matrix:\n",
      "[[119  22  87]\n",
      " [ 26  17  78]\n",
      " [ 48  17 776]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=0.9, class_weight={1: 0.4, 2: 0.4, 3: 0.2}, '\n",
      "               'dual=False, fit_intercept=True, intercept_scaling=1, '\n",
      "               'l1_ratio=None, max_iter=2000, multi_class=ovr, n_jobs=None, '\n",
      "               'penalty=l2, random_state=None, solver=saga, tol=0.0001, '\n",
      "               'verbose=0, warm_start=False, )',\n",
      " 'preprocessor': 'Preprocessor()',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n",
      "train time: 7.606s\n",
      "test time:  0.105s\n",
      "\n",
      "mcc:   0.478,   prec:   0.595,   rec:   0.573,   acc:   0.774\n",
      "\n",
      "confusion matrix:\n",
      "[[144  24  60]\n",
      " [ 28  23  70]\n",
      " [ 61  26 754]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=1.0, class_weight=None, dual=False, '\n",
      "               'fit_intercept=True, intercept_scaling=1, l1_ratio=None, '\n",
      "               'max_iter=100, multi_class=auto, n_jobs=None, penalty=l2, '\n",
      "               'random_state=None, solver=lbfgs, tol=0.0001, verbose=0, '\n",
      "               'warm_start=False, )',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amagill/.envs/38/fakenews/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.524s\n",
      "test time:  0.111s\n",
      "\n",
      "mcc:   0.402,   prec:   0.591,   rec:   0.528,   acc:   0.742\n",
      "\n",
      "confusion matrix:\n",
      "[[114  15  92]\n",
      " [ 31  24  94]\n",
      " [ 54  21 745]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=0.9, class_weight={1: 0.4, 2: 0.4, 3: 0.2}, '\n",
      "               'dual=False, fit_intercept=True, intercept_scaling=1, '\n",
      "               'l1_ratio=None, max_iter=2000, multi_class=ovr, n_jobs=None, '\n",
      "               'penalty=l2, random_state=None, solver=saga, tol=0.0001, '\n",
      "               'verbose=0, warm_start=False, )',\n",
      " 'preprocessor': 'Preprocessor()',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n",
      "train time: 8.082s\n",
      "test time:  0.115s\n",
      "\n",
      "mcc:   0.461,   prec:   0.609,   rec:   0.557,   acc:   0.762\n",
      "\n",
      "confusion matrix:\n",
      "[[138  11  72]\n",
      " [ 45  20  84]\n",
      " [ 54  17 749]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=1.0, class_weight=None, dual=False, '\n",
      "               'fit_intercept=True, intercept_scaling=1, l1_ratio=None, '\n",
      "               'max_iter=100, multi_class=auto, n_jobs=None, penalty=l2, '\n",
      "               'random_state=None, solver=lbfgs, tol=0.0001, verbose=0, '\n",
      "               'warm_start=False, )',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amagill/.envs/38/fakenews/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.618s\n",
      "test time:  0.120s\n",
      "\n",
      "mcc:   0.410,   prec:   0.604,   rec:   0.533,   acc:   0.745\n",
      "\n",
      "confusion matrix:\n",
      "[[109  19 102]\n",
      " [ 29  30  85]\n",
      " [ 47  22 747]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=0.9, class_weight={1: 0.4, 2: 0.4, 3: 0.2}, '\n",
      "               'dual=False, fit_intercept=True, intercept_scaling=1, '\n",
      "               'l1_ratio=None, max_iter=2000, multi_class=ovr, n_jobs=None, '\n",
      "               'penalty=l2, random_state=None, solver=saga, tol=0.0001, '\n",
      "               'verbose=0, warm_start=False, )',\n",
      " 'preprocessor': 'Preprocessor()',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n",
      "train time: 7.300s\n",
      "test time:  0.110s\n",
      "\n",
      "mcc:   0.459,   prec:   0.620,   rec:   0.577,   acc:   0.755\n",
      "\n",
      "confusion matrix:\n",
      "[[137  18  75]\n",
      " [ 34  35  75]\n",
      " [ 64  25 727]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=1.0, class_weight=None, dual=False, '\n",
      "               'fit_intercept=True, intercept_scaling=1, l1_ratio=None, '\n",
      "               'max_iter=100, multi_class=auto, n_jobs=None, penalty=l2, '\n",
      "               'random_state=None, solver=lbfgs, tol=0.0001, verbose=0, '\n",
      "               'warm_start=False, )',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amagill/.envs/38/fakenews/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.661s\n",
      "test time:  0.125s\n",
      "\n",
      "mcc:   0.375,   prec:   0.579,   rec:   0.515,   acc:   0.725\n",
      "\n",
      "confusion matrix:\n",
      "[[106  18  97]\n",
      " [ 34  26 100]\n",
      " [ 60  18 731]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=0.9, class_weight={1: 0.4, 2: 0.4, 3: 0.2}, '\n",
      "               'dual=False, fit_intercept=True, intercept_scaling=1, '\n",
      "               'l1_ratio=None, max_iter=2000, multi_class=ovr, n_jobs=None, '\n",
      "               'penalty=l2, random_state=None, solver=saga, tol=0.0001, '\n",
      "               'verbose=0, warm_start=False, )',\n",
      " 'preprocessor': 'Preprocessor()',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n",
      "train time: 6.733s\n",
      "test time:  0.104s\n",
      "\n",
      "mcc:   0.416,   prec:   0.577,   rec:   0.544,   acc:   0.735\n",
      "\n",
      "confusion matrix:\n",
      "[[129  18  74]\n",
      " [ 41  25  94]\n",
      " [ 63  25 721]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=1.0, class_weight=None, dual=False, '\n",
      "               'fit_intercept=True, intercept_scaling=1, l1_ratio=None, '\n",
      "               'max_iter=100, multi_class=auto, n_jobs=None, penalty=l2, '\n",
      "               'random_state=None, solver=lbfgs, tol=0.0001, verbose=0, '\n",
      "               'warm_start=False, )',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=False, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=None, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/amagill/.envs/38/fakenews/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train time: 1.570s\n",
      "test time:  0.116s\n",
      "\n",
      "mcc:   0.388,   prec:   0.559,   rec:   0.520,   acc:   0.739\n",
      "\n",
      "confusion matrix:\n",
      "[[106  23  83]\n",
      " [ 33  23  90]\n",
      " [ 54  27 751]]\n",
      "\n",
      "\n",
      "________________________________________________________________________________\n",
      "Training: \n",
      "{'classifier': 'LogisticRegression(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, C=0.9, class_weight={1: 0.4, 2: 0.4, 3: 0.2}, '\n",
      "               'dual=False, fit_intercept=True, intercept_scaling=1, '\n",
      "               'l1_ratio=None, max_iter=2000, multi_class=ovr, n_jobs=None, '\n",
      "               'penalty=l2, random_state=None, solver=saga, tol=0.0001, '\n",
      "               'verbose=0, warm_start=False, )',\n",
      " 'preprocessor': 'Preprocessor()',\n",
      " 'vectorizer': 'CountVectorizer(analyzer=word, binary=False, '\n",
      "               \"decode_error=strict, dtype=<class 'numpy.int64'>, \"\n",
      "               'encoding=utf-8, input=content, lowercase=True, max_df=1.0, '\n",
      "               'max_features=None, min_df=1, ngram_range=(1, 1), '\n",
      "               'preprocessor=None, stop_words=None, strip_accents=unicode, '\n",
      "               'token_pattern=(?u)\\\\b\\\\w\\\\w+\\\\b, tokenizer=None, '\n",
      "               'vocabulary=None, )'}\n",
      "train time: 8.488s\n",
      "test time:  0.106s\n",
      "\n",
      "mcc:   0.406,   prec:   0.559,   rec:   0.536,   acc:   0.740\n",
      "\n",
      "confusion matrix:\n",
      "[[124  17  71]\n",
      " [ 41  20  85]\n",
      " [ 70  25 737]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for split in splits:\n",
    "    for expirement in expirements:\n",
    "        rdf, X_test, predictions, probabilities = run(expirement, split)\n",
    "        results.append(rdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.concat(results)\n",
    "results_df = results_df.groupby(['expirement']).mean()\n",
    "results_df = results_df.sort_index(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_701fb340_3bb3_11eb_a863_acbc32933659 th {\n",
       "          height: 36px;\n",
       "          text-align: left;\n",
       "          font-size: 130%;\n",
       "          font-weight: bold;\n",
       "    }    #T_701fb340_3bb3_11eb_a863_acbc32933659 td {\n",
       "          height: 36px;\n",
       "          width: 160px;\n",
       "          font-size: 130%;\n",
       "    }    #T_701fb340_3bb3_11eb_a863_acbc32933659 td.col0 {\n",
       "          height: 36px;\n",
       "          width: 260px;\n",
       "          font-size: 130%;\n",
       "    }    #T_701fb340_3bb3_11eb_a863_acbc32933659 caption {\n",
       "          font-size: 130%;\n",
       "          height: 44px;\n",
       "          font-weight: bold;\n",
       "          padding-top: 30px;\n",
       "    }    #T_701fb340_3bb3_11eb_a863_acbc32933659 * {\n",
       "          background-color: #f0f0f0;\n",
       "    }#T_701fb340_3bb3_11eb_a863_acbc32933659row0_col1{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,coral 44.4%, transparent 44.4%);\n",
       "            font-weight:  bold;\n",
       "        }#T_701fb340_3bb3_11eb_a863_acbc32933659row0_col2{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,lightgrey 59.2%, transparent 59.2%);\n",
       "            font-weight:  bold;\n",
       "        }#T_701fb340_3bb3_11eb_a863_acbc32933659row0_col3{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,darkgrey 55.7%, transparent 55.7%);\n",
       "            font-weight:  bold;\n",
       "        }#T_701fb340_3bb3_11eb_a863_acbc32933659row1_col1{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,coral 40.2%, transparent 40.2%);\n",
       "        }#T_701fb340_3bb3_11eb_a863_acbc32933659row1_col2{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,lightgrey 58.3%, transparent 58.3%);\n",
       "        }#T_701fb340_3bb3_11eb_a863_acbc32933659row1_col3{\n",
       "            width:  10em;\n",
       "             height:  80%;\n",
       "            background:  linear-gradient(90deg,darkgrey 52.5%, transparent 52.5%);\n",
       "        }</style><table id=\"T_701fb340_3bb3_11eb_a863_acbc32933659\" ><caption>Classifier Performance</caption><thead>    <tr>        <th class=\"col_heading level0 col0\" >expirement</th>        <th class=\"col_heading level0 col1\" >mcc</th>        <th class=\"col_heading level0 col2\" >precision</th>        <th class=\"col_heading level0 col3\" >recall</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                                <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row0_col0\" class=\"data row0 col0\" >001</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row0_col1\" class=\"data row0 col1\" >0.44</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row0_col2\" class=\"data row0 col2\" >0.59</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row0_col3\" class=\"data row0 col3\" >0.56</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                                <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row1_col0\" class=\"data row1 col0\" >baseline</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row1_col1\" class=\"data row1 col1\" >0.40</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row1_col2\" class=\"data row1 col2\" >0.58</td>\n",
       "                        <td id=\"T_701fb340_3bb3_11eb_a863_acbc32933659row1_col3\" class=\"data row1 col3\" >0.52</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "make_table(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
