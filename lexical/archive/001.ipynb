{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import emoji\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from random import randint\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef, precision_score, \n",
    "                             precision_recall_fscore_support, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# generate file paths to data #\n",
    "###############################\n",
    "\n",
    "hydrated_tweet_folder = os.path.join(\"..\",\"output\",\"data\")\n",
    "tweet_ids_folder = os.path.join(\"..\",\"output\",\"data\")\n",
    "\n",
    "path_5g_json = \"5g_corona_conspiracy.json\"\n",
    "path_other_json = \"other_conspiracy.json\"\n",
    "path_non_consp_json = \"non_conspiracy.json\"\n",
    "path_test_json = \"test_tweets.json\"\n",
    "path_test_ids_txt = \"test_tweet_ids.json\"\n",
    "\n",
    "path_5g = os.path.join(hydrated_tweet_folder, path_5g_json)\n",
    "path_other = os.path.join(hydrated_tweet_folder, path_other_json)\n",
    "path_non = os.path.join(hydrated_tweet_folder, path_non_consp_json)\n",
    "path_test = os.path.join(hydrated_tweet_folder, path_test_json)\n",
    "path_test_ids = os.path.join(tweet_ids_folder, path_test_ids_txt)\n",
    "\n",
    "assert(os.path.isfile(path_5g))\n",
    "assert(os.path.isfile(path_other))\n",
    "assert(os.path.isfile(path_non))\n",
    "assert(os.path.isfile(path_test))\n",
    "assert(os.path.isfile(path_test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# read in data #\n",
    "################\n",
    "\n",
    "fiveg_df = pd.read_json(path_5g)\n",
    "other_df = pd.read_json(path_other)\n",
    "nocon_df = pd.read_json(path_non)\n",
    "test_df = pd.read_json(path_test)\n",
    "\n",
    "# we will need to submit predictions for all tweet ids\n",
    "# test_ids_df = pd.read_csv(path_test_ids, names=['id'])\n",
    "test_ids_df = pd.read_json(path_test_ids)\n",
    "test_ids_df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "test_id_set = set(test_ids_df['id'])\n",
    "retreived_test_set = set(test_df['id'])\n",
    "\n",
    "# find missing tweets from test set\n",
    "missing_test_tweets = test_id_set.difference(retreived_test_set)\n",
    "\n",
    "# mark as real tweets, because we're going to add fake tweets later\n",
    "fiveg_df['actual_tweet'] = True\n",
    "other_df['actual_tweet'] = True\n",
    "nocon_df['actual_tweet'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# train eval split #\n",
    "####################\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "def mark_train(df, train_ratio=0.8, test_ids=None):\n",
    "    \n",
    "    if test_ids:\n",
    "        df['test'] = df.apply(lambda row:(str(row['id']) in test_ids) and row['actual_tweet'], axis=1)\n",
    "    else:\n",
    "        df['test'] = df.apply(lambda row: (randint(1,100) > int(train_ratio*100) and row['actual_tweet']), axis=1)            \n",
    "        \n",
    "    return df\n",
    "\n",
    "fiveg_df = mark_train(fiveg_df, train_ratio=train_ratio)\n",
    "other_df = mark_train(other_df, train_ratio=train_ratio)\n",
    "nocon_df = mark_train(nocon_df, train_ratio=train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            train         test       train pct\n",
      "\n",
      "FIVEG:        908          212           0.81%\n",
      "OTHER:        566          122           0.82%\n",
      "NOCON:      3,307          831           0.80%\n",
      "TOTAL:      4,781        1,165           0.80%\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# label and concat #\n",
    "####################\n",
    "\n",
    "fiveg_df['label'] = 1\n",
    "other_df['label'] = 2\n",
    "nocon_df['label'] = 3\n",
    "\n",
    "print(f\"\\n{'train':>17} {'test':>12} {'train pct':>15}\\n\")\n",
    "\n",
    "def display_ratio(df, name):\n",
    "    eval_df = df[df['test']==True]\n",
    "    train_df = df[df['test']==False]\n",
    "    \n",
    "    print(f'{name}: {len(train_df):>10,} {len(eval_df):>12,} {len(train_df)/len(df):>14.2f}%')\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "fiveg_train_df, _ = display_ratio(fiveg_df, 'FIVEG')\n",
    "other_train_df, _ = display_ratio(other_df, 'OTHER')\n",
    "nocon_train_df, _ = display_ratio(nocon_df, 'NOCON')\n",
    "\n",
    "df = pd.concat([fiveg_df, other_df, nocon_df])\n",
    "\n",
    "train_df, eval_df = display_ratio(df, 'TOTAL')\n",
    "\n",
    "X_train = train_df['full_text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_eval = eval_df['full_text']\n",
    "y_eval = eval_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_test = False\n",
    "if no_test :\n",
    "    X_train = X_train.append(X_eval)\n",
    "    y_train = y_train.append(y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# preprocessing #\n",
    "#################\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.re_prog_url = re.compile(r'https://t.co/([a-zA-Z0-9]+)')\n",
    "    \n",
    "    def fit( self, X, y=None ):\n",
    "        return self \n",
    "    \n",
    "    def _process(self, text):\n",
    "        \n",
    "        urls = self.re_prog_url.findall(text)\n",
    "        text = text.lower()\\\n",
    "                .replace('https://t.co/', '')\\\n",
    "                .replace('u.s.', 'us')\\\n",
    "                .replace('u.k.', 'uk')\n",
    "        for url in urls:\n",
    "            text = text.replace(url.lower(), 'url')\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X = X.apply(self._process)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "\n",
      "Accuracy  Precision  Recall   F1       MCC\n",
      "95.28%    93.35%     91.96%   92.62%   89.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# pipeline #\n",
    "############\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    C=0.9,\n",
    "    class_weight={\n",
    "        1: 0.4,\n",
    "        2: 0.4,\n",
    "        3: 0.2\n",
    "    },\n",
    "    multi_class= 'ovr',\n",
    "    max_iter=2000,\n",
    "    solver= 'saga'\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', Preprocessor()),\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', classifier)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_eval)\n",
    "probabilities = pipeline.predict_proba(X_eval)\n",
    "\n",
    "accuracy = accuracy_score(y_eval, predictions)*100\n",
    "precision = precision_score(y_eval, predictions, zero_division=0, average=\"macro\")*100\n",
    "recall = recall_score(y_eval, predictions, average=\"macro\")*100\n",
    "f1 = f1_score(y_eval, predictions, average=\"macro\")*100\n",
    "support = precision_recall_fscore_support(y_eval, predictions, average=\"macro\")\n",
    "matthews = matthews_corrcoef(y_eval, predictions)*100\n",
    "\n",
    "header = classifier.__class__.__name__\n",
    "\n",
    "print(f'\\n{header}\\n\\nAccuracy  Precision  Recall   F1       MCC')\n",
    "print(f'{accuracy:.2f}%{precision:>9.2f}%{recall:>10.2f}%{f1:>8.2f}%{matthews:>8.2f}%\\n')\n",
    "\n",
    "##############\n",
    "# submission #\n",
    "##############\n",
    "\n",
    "predictions = pipeline.predict(test_df['full_text'])\n",
    "probabilities = pipeline.predict_proba(test_df['full_text'])\n",
    "\n",
    "filename = os.path.join(\"..\",\"output\",\"ME20FND_DL-TXST_001.txt\")\n",
    "if no_test:\n",
    "    filename = os.path.join(\"..\",\"output\",\"ME20FND_DL-TXST_001b.txt\")\n",
    "\n",
    "threshold = 0.10\n",
    "    \n",
    "with open(filename,'w') as f:\n",
    "    for tweet_id, prediction, prob in zip(test_df['id'], predictions, probabilities):\n",
    "        \n",
    "        diff = prob[prediction-1]-st.median(prob)\n",
    "        if diff < threshold:\n",
    "            prediction = 0\n",
    "        \n",
    "        f.write(f'{tweet_id},{prediction}\\n')\n",
    "    for tweet_id in missing_test_tweets:\n",
    "        f.write(f'{tweet_id},-1\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "75.99%    56.19%     54.50%   54.50%   45.66%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "74.86%    56.45%     53.92%   54.73%   42.48%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "75.68%    58.08%     55.35%   56.16%   44.05%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "75.60%    57.98%     55.21%   56.04%   43.80%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "75.52%    57.61%     54.94%   55.71%   43.61%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       MCC\n",
    "75.60%    57.41%     54.52%   55.27%   43.53%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       Matthews\n",
    "75.52%    57.76%     52.16%   53.90%   40.90%\n",
    "\n",
    "SVC\n",
    "\n",
    "Accuracy  Precision  Recall   F1       Matthews\n",
    "73.40%    55.05%     54.75%   54.82%   39.60%\n",
    "\n",
    "LogisticRegression\n",
    "\n",
    "Accuracy  Precision  Recall   F1       Matthews\n",
    "77.58%    59.50%     54.57%   55.99%   45.10%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                        #####################\n",
    "                                        # stop here for now #\n",
    "                                        #####################\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIVEG:\n",
      "\n",
      "everywhere\n",
      "design\n",
      "map\n",
      "out\n",
      "killing\n",
      "cursing\n",
      "therefore\n",
      "harms\n",
      "28\n",
      "place\n",
      "popping\n",
      "function\n",
      "whistleblower\n",
      "spots\n",
      "loss\n",
      "alongside\n",
      "areas\n",
      "#coronavirus\n",
      "structure\n",
      "linked\n",
      "boss\n",
      "coverage\n",
      "testing\n",
      "discussing\n",
      "ðŸ¤¦ðŸ½â€â™‚ï¸\n",
      "putting\n",
      "issues\n",
      "nwo\n",
      "mentions\n",
      "flying\n",
      "created\n",
      "die\n",
      "know\n",
      "@worldtruthtv\n",
      "aka\n",
      "turned\n",
      "project\n",
      "illness\n",
      "cv\n",
      "microwave\n",
      "frequency\n",
      "former\n",
      "chemtrails\n",
      "ig\n",
      "connected\n",
      "cells\n",
      "version\n",
      "distancing\n",
      "kill\n",
      "#5gtowers\n",
      "weapon\n",
      "share\n",
      "towers\n",
      "liberty\n",
      "video\n",
      "okay\n",
      "#5gkills\n",
      "exposure\n",
      "correlation\n",
      "london\n",
      "ainâ€™t\n",
      "govt\n",
      "destroy\n",
      "watch\n",
      "radiation\n",
      "21\n",
      "used\n",
      "night\n",
      "side\n",
      "depopulation\n",
      "alter\n",
      "connection\n",
      "david\n",
      "#covid\n",
      "lot\n",
      "order\n",
      "play\n",
      "dying\n",
      "flu\n",
      "hemoglobin\n",
      "city\n",
      "tell\n",
      "activating\n",
      "agenda\n",
      "emf\n",
      "electromagnetic\n",
      "truth\n",
      "research\n",
      "link\n",
      "rolled\n",
      "body\n",
      "chris\n",
      "question\n",
      "#5g\n",
      "coincidence\n",
      "oxygen\n",
      "immune\n",
      "5g\n",
      "symptoms\n",
      "wuhan\n",
      "\n",
      "OTHER:\n",
      "\n",
      "evil\n",
      "sacrifices\n",
      "ostensibly\n",
      "tracked\n",
      "hold\n",
      "played\n",
      "predicted\n",
      "ppl\n",
      "chip\n",
      "confident\n",
      "ireland\n",
      "got\n",
      "eye\n",
      "kids\n",
      "right\n",
      "they're\n",
      "war\n",
      "main\n",
      "digital\n",
      "censoring\n",
      "000\n",
      "frequencies\n",
      "mike\n",
      "#qanon\n",
      "2017\n",
      "down\n",
      "lie\n",
      "steele\n",
      "studies\n",
      "funny\n",
      "getting\n",
      "military\n",
      "gates\n",
      "being\n",
      "responsible\n",
      "effects\n",
      "sleeping\n",
      "control\n",
      "something\n",
      "released\n",
      "update\n",
      "boy\n",
      "none\n",
      "years\n",
      "necessary\n",
      "reason\n",
      "fuck\n",
      "interested\n",
      "made\n",
      "worry\n",
      "they'll\n",
      "arenâ€™t\n",
      "theres\n",
      "american\n",
      "put\n",
      "africa\n",
      "don't\n",
      "weather\n",
      "sounds\n",
      "schools\n",
      "government\n",
      "absurd\n",
      "shocking\n",
      "safety\n",
      "else\n",
      "prayers\n",
      "track\n",
      "pretext\n",
      "liz\n",
      "masters\n",
      "electro\n",
      "every\n",
      "familiar\n",
      "population\n",
      "thoughts\n",
      "guys\n",
      "fauci\n",
      "came\n",
      "doctors\n",
      "humans\n",
      "shape\n",
      "obviously\n",
      "but\n",
      "behind\n",
      "id2020\n",
      "children\n",
      "health\n",
      "can't\n",
      "pushed\n",
      "usa\n",
      "felt\n",
      "bad\n",
      "adverse\n",
      "msm\n",
      "during\n",
      "#stop5g\n",
      "bioweapon\n",
      "installed\n",
      "cancer\n",
      "lab\n",
      "\n",
      "NOCON:\n",
      "\n",
      "uv\n",
      "thanks\n",
      "develop\n",
      "wtf\n",
      "videos\n",
      "bleach\n",
      "prevent\n",
      "step\n",
      "experts\n",
      "ridiculous\n",
      "reported\n",
      "gonna\n",
      "spreads\n",
      "causes\n",
      "situation\n",
      "hear\n",
      "back\n",
      "work\n",
      "measures\n",
      "better\n",
      "tried\n",
      "linking\n",
      "burned\n",
      "healthcare\n",
      "total\n",
      "rays\n",
      "believe\n",
      "says\n",
      "hope\n",
      "nhs\n",
      "logic\n",
      "hoax\n",
      "come\n",
      "30\n",
      "impact\n",
      "killed\n",
      "#huawei\n",
      "shut\n",
      "false\n",
      "you're\n",
      "masts\n",
      "company\n",
      "among\n",
      "united\n",
      "cause\n",
      "broadband\n",
      "much\n",
      "5\n",
      "despite\n",
      "infrastructure\n",
      "statement\n",
      "national\n",
      "someone\n",
      "hospital\n",
      "absolutely\n",
      "gives\n",
      "viral\n",
      "antichrist\n",
      "misinformation\n",
      "person\n",
      "t\n",
      "stupidity\n",
      "claiming\n",
      "pray\n",
      "spread\n",
      "fire\n",
      "because\n",
      "thereâ€™s\n",
      "telling\n",
      "social\n",
      "stupid\n",
      "app\n",
      "today's\n",
      "seeing\n",
      "britain\n",
      "huawei\n",
      "believes\n",
      "folks\n",
      "think\n",
      "countries\n",
      "south\n",
      "mobile\n",
      "her\n",
      "day\n",
      "does\n",
      "youtube\n",
      "early\n",
      "idea\n",
      "mean\n",
      "thinking\n",
      "claim\n",
      "crisis\n",
      "conspiracies\n",
      "whatsapp\n",
      "facebook\n",
      "thinks\n",
      "networks\n",
      "conspiracy\n",
      "next\n",
      "burning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = {1:'FIVEG',2:'OTHER',3:'NOCON'}\n",
    "\n",
    "def print_top100(vectorizer, clf, class_labels):    \n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top100 = np.argsort(clf.coef_[i])[-100:]\n",
    "        print(\"%s:\\n\\n%s\\n\" % (m[class_label],\n",
    "              \"\\n\".join(feature_names[j] for j in top100)))\n",
    "        \n",
    "print_top100(vectorizer, classifier, [1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
