{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import emoji\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from random import randint\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef, precision_score, \n",
    "                             precision_recall_fscore_support, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# generate file paths to data #\n",
    "###############################\n",
    "\n",
    "hydrated_tweet_folder = \"data\"\n",
    "tweet_ids_folder = \"data\"\n",
    "\n",
    "path_5g_json = \"5g_corona_conspiracy.json\"\n",
    "path_other_json = \"other_conspiracy.json\"\n",
    "path_non_consp_json = \"non_conspiracy.json\"\n",
    "path_test_json = \"test_tweets.json\"\n",
    "path_test_ids_txt = \"test_tweet_ids.json\"\n",
    "\n",
    "path_5g = os.path.join(hydrated_tweet_folder, path_5g_json)\n",
    "path_other = os.path.join(hydrated_tweet_folder, path_other_json)\n",
    "path_non = os.path.join(hydrated_tweet_folder, path_non_consp_json)\n",
    "path_test = os.path.join(hydrated_tweet_folder, path_test_json)\n",
    "path_test_ids = os.path.join(tweet_ids_folder, path_test_ids_txt)\n",
    "\n",
    "assert(os.path.isfile(path_5g))\n",
    "assert(os.path.isfile(path_other))\n",
    "assert(os.path.isfile(path_non))\n",
    "assert(os.path.isfile(path_test))\n",
    "assert(os.path.isfile(path_test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# read in data #\n",
    "################\n",
    "\n",
    "fiveg_df = pd.read_json(path_5g)\n",
    "other_df = pd.read_json(path_other)\n",
    "nocon_df = pd.read_json(path_non)\n",
    "test_df = pd.read_json(path_test)\n",
    "\n",
    "# we will need to submit predictions for all tweet ids\n",
    "# test_ids_df = pd.read_csv(path_test_ids, names=['id'])\n",
    "test_ids_df = pd.read_json(path_test_ids)\n",
    "test_ids_df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "test_id_set = set(test_ids_df['id'])\n",
    "retreived_test_set = set(test_df['id'])\n",
    "\n",
    "# find missing tweets from test set\n",
    "missing_test_tweets = test_id_set.difference(retreived_test_set)\n",
    "\n",
    "# mark as real tweets, because we're going to add fake tweets later\n",
    "fiveg_df['actual_tweet'] = True\n",
    "other_df['actual_tweet'] = True\n",
    "nocon_df['actual_tweet'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "263\n"
     ]
    }
   ],
   "source": [
    "#set(test_df['id']).intersection(set(df['id']))\n",
    "#set(df['id']).intersection(set(test_df['id']))\n",
    "print(len(set(test_df['id']).difference(test_id_set)))\n",
    "print(len(set(test_id_set).difference(test_df['id'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# train eval split #\n",
    "####################\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "def mark_train(df, train_ratio=0.8, test_ids=None):\n",
    "    \n",
    "    if test_ids:\n",
    "        df['test'] = df.apply(lambda row:(str(row['id']) in test_ids) and row['actual_tweet'], axis=1)\n",
    "    else:\n",
    "        df['test'] = df.apply(lambda row: (randint(1,100) > int(train_ratio*100) and row['actual_tweet']), axis=1)            \n",
    "        \n",
    "    return df\n",
    "\n",
    "fiveg_df = mark_train(fiveg_df, train_ratio=train_ratio)\n",
    "other_df = mark_train(other_df, train_ratio=train_ratio)\n",
    "nocon_df = mark_train(nocon_df, train_ratio=train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            train         test       train pct\n",
      "\n",
      "FIVEG:        901          219           0.80%\n",
      "OTHER:        552          136           0.80%\n",
      "NOCON:      3,328          810           0.80%\n",
      "TOTAL:      4,781        1,165           0.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6c3b8cab07a3>:56: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_df['full_text'] = train_df.apply(add_ocr, axis=1)\n",
      "<ipython-input-6-6c3b8cab07a3>:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  eval_df['full_text'] = eval_df.apply(add_ocr, axis=1)\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# label and concat #\n",
    "####################\n",
    "\n",
    "fiveg_df['label'] = 1\n",
    "other_df['label'] = 0\n",
    "nocon_df['label'] = 0\n",
    "\n",
    "print(f\"\\n{'train':>17} {'test':>12} {'train pct':>15}\\n\")\n",
    "\n",
    "def display_ratio(df, name):\n",
    "    eval_df = df[df['test']==True]\n",
    "    train_df = df[df['test']==False]\n",
    "    \n",
    "    print(f'{name}: {len(train_df):>10,} {len(eval_df):>12,} {len(train_df)/len(df):>14.2f}%')\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "fiveg_train_df, _ = display_ratio(fiveg_df, 'FIVEG')\n",
    "other_train_df, _ = display_ratio(other_df, 'OTHER')\n",
    "nocon_train_df, _ = display_ratio(nocon_df, 'NOCON')\n",
    "\n",
    "df = pd.concat([fiveg_df, other_df, nocon_df])\n",
    "\n",
    "train_df, eval_df = display_ratio(df, 'TOTAL')\n",
    "\n",
    "############\n",
    "# join ocr #\n",
    "############\n",
    "\n",
    "fiveg_ocr = pd.read_csv('output/image_terms_fiveg.csv')\n",
    "nocon_ocr = pd.read_csv('output/image_terms_nocon.csv')\n",
    "other_ocr = pd.read_csv('output/image_terms_other.csv')\n",
    "test_ocr = pd.read_csv('output/image_terms_test.csv')\n",
    "\n",
    "ocr_df = pd.concat([\n",
    "    fiveg_ocr,\n",
    "    nocon_ocr,\n",
    "    other_ocr,\n",
    "    test_ocr\n",
    "])\n",
    "\n",
    "def add_ocr(row):\n",
    "    if 'media' in row.entities:\n",
    "        x = ocr_df[ocr_df['filename'] == str(row.id)+'.png']\n",
    "        \n",
    "        try:\n",
    "            terms = x['terms'].iloc[0]\n",
    "            if type(terms) == str:\n",
    "                row['full_text'] += terms\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return row['full_text']\n",
    "\n",
    "train_df['full_text'] = train_df.apply(add_ocr, axis=1)\n",
    "eval_df['full_text'] = eval_df.apply(add_ocr, axis=1)\n",
    "test_df['full_text'] = test_df.apply(add_ocr, axis=1)\n",
    "\n",
    "########################\n",
    "# prepare for training #\n",
    "########################\n",
    "\n",
    "X_train = train_df['full_text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_eval = eval_df['full_text']\n",
    "y_eval = eval_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_test = False\n",
    "if no_test :\n",
    "    X_train = X_train.append(X_eval)\n",
    "    y_train = y_train.append(y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# preprocessing #\n",
    "#################\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.re_prog_url = re.compile(r'https://t.co/([a-zA-Z0-9]+)')\n",
    "    \n",
    "    def fit( self, X, y=None ):\n",
    "        return self \n",
    "    \n",
    "    def _process(self, text):\n",
    "        \n",
    "        urls = self.re_prog_url.findall(text)\n",
    "        text = text.lower()\\\n",
    "                .replace('https://t.co/', '')\\\n",
    "                .replace('u.s.', 'us')\\\n",
    "                .replace('u.k.', 'uk')\\\n",
    "                .replace('5 g', '5g')\n",
    "        for url in urls:\n",
    "            text = text.replace(url.lower(), 'url')\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X = X.apply(self._process)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "\n",
      "Accuracy  Precision  Recall   F1       MCC\n",
      "97.51%    96.46%     95.31%   95.87%   91.76%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# pipeline #\n",
    "############\n",
    "\n",
    "class_weights={\n",
    "    0: 0.4,\n",
    "    1: 0.6\n",
    "}\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    # C=0.9,\n",
    "    class_weight=class_weights,\n",
    "    # multi_class= 'ovr',\n",
    "    max_iter=2000,\n",
    "    solver= 'saga'\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode'\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', Preprocessor()),\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', classifier)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_eval)\n",
    "probabilities = pipeline.predict_proba(X_eval)\n",
    "\n",
    "accuracy = accuracy_score(y_eval, predictions)*100\n",
    "precision = precision_score(y_eval, predictions, zero_division=0, average=\"macro\")*100\n",
    "recall = recall_score(y_eval, predictions, average=\"macro\")*100\n",
    "f1 = f1_score(y_eval, predictions, average=\"macro\")*100\n",
    "support = precision_recall_fscore_support(y_eval, predictions, average=\"macro\")\n",
    "matthews = matthews_corrcoef(y_eval, predictions)*100\n",
    "\n",
    "header = classifier.__class__.__name__\n",
    "\n",
    "print(f'\\n{header}\\n\\nAccuracy  Precision  Recall   F1       MCC')\n",
    "print(f'{accuracy:.2f}%{precision:>9.2f}%{recall:>10.2f}%{f1:>8.2f}%{matthews:>8.2f}%\\n')\n",
    "\n",
    "##############\n",
    "# submission #\n",
    "##############\n",
    "\n",
    "predictions = pipeline.predict(test_df['full_text'])\n",
    "probabilities = pipeline.predict_proba(test_df['full_text'])\n",
    "\n",
    "filename = os.path.join('output','ME20FND_DL-TXST_012.txt')\n",
    "if no_test:\n",
    "    filename = os.path.join('output','ME20FND_DL-TXST_012b.txt')\n",
    "\n",
    "with open(filename,'w') as f:\n",
    "    for tweet_id, prediction, prob in zip(test_df['id'], predictions, probabilities):        \n",
    "        f.write(f'{tweet_id},{prediction}\\n')\n",
    "    for tweet_id in missing_test_tweets:\n",
    "        f.write(f'{tweet_id},-1\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                        #####################\n",
    "                                        # stop here for now #\n",
    "                                        #####################\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIVEG:\n",
      "\n",
      "everywhere\n",
      "design\n",
      "map\n",
      "out\n",
      "killing\n",
      "cursing\n",
      "therefore\n",
      "harms\n",
      "28\n",
      "place\n",
      "popping\n",
      "function\n",
      "whistleblower\n",
      "spots\n",
      "loss\n",
      "alongside\n",
      "areas\n",
      "#coronavirus\n",
      "structure\n",
      "linked\n",
      "boss\n",
      "coverage\n",
      "testing\n",
      "discussing\n",
      "ü§¶üèΩ‚Äç‚ôÇÔ∏è\n",
      "putting\n",
      "issues\n",
      "nwo\n",
      "mentions\n",
      "flying\n",
      "created\n",
      "die\n",
      "know\n",
      "@worldtruthtv\n",
      "aka\n",
      "turned\n",
      "project\n",
      "illness\n",
      "cv\n",
      "microwave\n",
      "frequency\n",
      "former\n",
      "chemtrails\n",
      "ig\n",
      "connected\n",
      "cells\n",
      "version\n",
      "distancing\n",
      "kill\n",
      "#5gtowers\n",
      "weapon\n",
      "share\n",
      "towers\n",
      "liberty\n",
      "video\n",
      "okay\n",
      "#5gkills\n",
      "exposure\n",
      "correlation\n",
      "london\n",
      "ain‚Äôt\n",
      "govt\n",
      "destroy\n",
      "watch\n",
      "radiation\n",
      "21\n",
      "used\n",
      "night\n",
      "side\n",
      "depopulation\n",
      "alter\n",
      "connection\n",
      "david\n",
      "#covid\n",
      "lot\n",
      "order\n",
      "play\n",
      "dying\n",
      "flu\n",
      "hemoglobin\n",
      "city\n",
      "tell\n",
      "activating\n",
      "agenda\n",
      "emf\n",
      "electromagnetic\n",
      "truth\n",
      "research\n",
      "link\n",
      "rolled\n",
      "body\n",
      "chris\n",
      "question\n",
      "#5g\n",
      "coincidence\n",
      "oxygen\n",
      "immune\n",
      "5g\n",
      "symptoms\n",
      "wuhan\n",
      "\n",
      "OTHER:\n",
      "\n",
      "evil\n",
      "sacrifices\n",
      "ostensibly\n",
      "tracked\n",
      "hold\n",
      "played\n",
      "predicted\n",
      "ppl\n",
      "chip\n",
      "confident\n",
      "ireland\n",
      "got\n",
      "eye\n",
      "kids\n",
      "right\n",
      "they're\n",
      "war\n",
      "main\n",
      "digital\n",
      "censoring\n",
      "000\n",
      "frequencies\n",
      "mike\n",
      "#qanon\n",
      "2017\n",
      "down\n",
      "lie\n",
      "steele\n",
      "studies\n",
      "funny\n",
      "getting\n",
      "military\n",
      "gates\n",
      "being\n",
      "responsible\n",
      "effects\n",
      "sleeping\n",
      "control\n",
      "something\n",
      "released\n",
      "update\n",
      "boy\n",
      "none\n",
      "years\n",
      "necessary\n",
      "reason\n",
      "fuck\n",
      "interested\n",
      "made\n",
      "worry\n",
      "they'll\n",
      "aren‚Äôt\n",
      "theres\n",
      "american\n",
      "put\n",
      "africa\n",
      "don't\n",
      "weather\n",
      "sounds\n",
      "schools\n",
      "government\n",
      "absurd\n",
      "shocking\n",
      "safety\n",
      "else\n",
      "prayers\n",
      "track\n",
      "pretext\n",
      "liz\n",
      "masters\n",
      "electro\n",
      "every\n",
      "familiar\n",
      "population\n",
      "thoughts\n",
      "guys\n",
      "fauci\n",
      "came\n",
      "doctors\n",
      "humans\n",
      "shape\n",
      "obviously\n",
      "but\n",
      "behind\n",
      "id2020\n",
      "children\n",
      "health\n",
      "can't\n",
      "pushed\n",
      "usa\n",
      "felt\n",
      "bad\n",
      "adverse\n",
      "msm\n",
      "during\n",
      "#stop5g\n",
      "bioweapon\n",
      "installed\n",
      "cancer\n",
      "lab\n",
      "\n",
      "NOCON:\n",
      "\n",
      "uv\n",
      "thanks\n",
      "develop\n",
      "wtf\n",
      "videos\n",
      "bleach\n",
      "prevent\n",
      "step\n",
      "experts\n",
      "ridiculous\n",
      "reported\n",
      "gonna\n",
      "spreads\n",
      "causes\n",
      "situation\n",
      "hear\n",
      "back\n",
      "work\n",
      "measures\n",
      "better\n",
      "tried\n",
      "linking\n",
      "burned\n",
      "healthcare\n",
      "total\n",
      "rays\n",
      "believe\n",
      "says\n",
      "hope\n",
      "nhs\n",
      "logic\n",
      "hoax\n",
      "come\n",
      "30\n",
      "impact\n",
      "killed\n",
      "#huawei\n",
      "shut\n",
      "false\n",
      "you're\n",
      "masts\n",
      "company\n",
      "among\n",
      "united\n",
      "cause\n",
      "broadband\n",
      "much\n",
      "5\n",
      "despite\n",
      "infrastructure\n",
      "statement\n",
      "national\n",
      "someone\n",
      "hospital\n",
      "absolutely\n",
      "gives\n",
      "viral\n",
      "antichrist\n",
      "misinformation\n",
      "person\n",
      "t\n",
      "stupidity\n",
      "claiming\n",
      "pray\n",
      "spread\n",
      "fire\n",
      "because\n",
      "there‚Äôs\n",
      "telling\n",
      "social\n",
      "stupid\n",
      "app\n",
      "today's\n",
      "seeing\n",
      "britain\n",
      "huawei\n",
      "believes\n",
      "folks\n",
      "think\n",
      "countries\n",
      "south\n",
      "mobile\n",
      "her\n",
      "day\n",
      "does\n",
      "youtube\n",
      "early\n",
      "idea\n",
      "mean\n",
      "thinking\n",
      "claim\n",
      "crisis\n",
      "conspiracies\n",
      "whatsapp\n",
      "facebook\n",
      "thinks\n",
      "networks\n",
      "conspiracy\n",
      "next\n",
      "burning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# m = {1:'FIVEG',2:'OTHER',3:'NOCON'}\n",
    "m = {0:'OTHER', 1:'FIVEG'}\n",
    "\n",
    "def print_top100(vectorizer, clf, class_labels):    \n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top100 = np.argsort(clf.coef_[i])[-100:]\n",
    "        print(\"%s:\\n\\n%s\\n\" % (m[class_label],\n",
    "              \"\\n\".join(feature_names[j] for j in top100)))\n",
    "        \n",
    "print_top100(vectorizer, classifier, [0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
