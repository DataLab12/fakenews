{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import emoji\n",
    "import math\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "from random import randint\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import (accuracy_score, f1_score, matthews_corrcoef, precision_score, \n",
    "                             precision_recall_fscore_support, recall_score, roc_auc_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm\n",
    "import statistics as st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "# generate file paths to data #\n",
    "###############################\n",
    "\n",
    "hydrated_tweet_folder = \"data\"\n",
    "tweet_ids_folder = \"data\"\n",
    "\n",
    "path_5g_json = \"5g_corona_conspiracy.json\"\n",
    "path_other_json = \"other_conspiracy.json\"\n",
    "path_non_consp_json = \"non_conspiracy.json\"\n",
    "path_test_json = \"test_tweets.json\"\n",
    "path_test_ids_txt = \"test_tweet_ids.json\"\n",
    "\n",
    "path_5g = os.path.join(hydrated_tweet_folder, path_5g_json)\n",
    "path_other = os.path.join(hydrated_tweet_folder, path_other_json)\n",
    "path_non = os.path.join(hydrated_tweet_folder, path_non_consp_json)\n",
    "path_test = os.path.join(hydrated_tweet_folder, path_test_json)\n",
    "path_test_ids = os.path.join(tweet_ids_folder, path_test_ids_txt)\n",
    "\n",
    "assert(os.path.isfile(path_5g))\n",
    "assert(os.path.isfile(path_other))\n",
    "assert(os.path.isfile(path_non))\n",
    "assert(os.path.isfile(path_test))\n",
    "assert(os.path.isfile(path_test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# read in data #\n",
    "################\n",
    "\n",
    "fiveg_df = pd.read_json(path_5g)\n",
    "other_df = pd.read_json(path_other)\n",
    "nocon_df = pd.read_json(path_non)\n",
    "test_df = pd.read_json(path_test)\n",
    "\n",
    "# we will need to submit predictions for all tweet ids\n",
    "# test_ids_df = pd.read_csv(path_test_ids, names=['id'])\n",
    "test_ids_df = pd.read_json(path_test_ids)\n",
    "test_ids_df.rename(columns = {0: 'id'}, inplace = True)\n",
    "\n",
    "test_id_set = set(test_ids_df['id'])\n",
    "retreived_test_set = set(test_df['id'])\n",
    "\n",
    "# find missing tweets from test set\n",
    "missing_test_tweets = test_id_set.difference(retreived_test_set)\n",
    "\n",
    "# mark as real tweets, because we're going to add fake tweets later\n",
    "fiveg_df['actual_tweet'] = True\n",
    "other_df['actual_tweet'] = True\n",
    "nocon_df['actual_tweet'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# train eval split #\n",
    "####################\n",
    "\n",
    "train_ratio = 0.8\n",
    "\n",
    "def mark_train(df, train_ratio=0.8, test_ids=None):\n",
    "    \n",
    "    if test_ids:\n",
    "        df['test'] = df.apply(lambda row:(str(row['id']) in test_ids) and row['actual_tweet'], axis=1)\n",
    "    else:\n",
    "        df['test'] = df.apply(lambda row: (randint(1,100) > int(train_ratio*100) and row['actual_tweet']), axis=1)            \n",
    "        \n",
    "    return df\n",
    "\n",
    "fiveg_df = mark_train(fiveg_df, train_ratio=train_ratio)\n",
    "other_df = mark_train(other_df, train_ratio=train_ratio)\n",
    "nocon_df = mark_train(nocon_df, train_ratio=train_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            train         test       train pct\n",
      "\n",
      "FIVEG:        868          252           0.78%\n",
      "OTHER:        529          159           0.77%\n",
      "NOCON:      3,295          843           0.80%\n",
      "TOTAL:      4,692        1,254           0.79%\n"
     ]
    }
   ],
   "source": [
    "####################\n",
    "# label and concat #\n",
    "####################\n",
    "\n",
    "fiveg_df['label'] = 1\n",
    "other_df['label'] = 2\n",
    "nocon_df['label'] = 3\n",
    "\n",
    "print(f\"\\n{'train':>17} {'test':>12} {'train pct':>15}\\n\")\n",
    "\n",
    "def display_ratio(df, name):\n",
    "    eval_df = df[df['test']==True]\n",
    "    train_df = df[df['test']==False]\n",
    "    \n",
    "    print(f'{name}: {len(train_df):>10,} {len(eval_df):>12,} {len(train_df)/len(df):>14.2f}%')\n",
    "    \n",
    "    return train_df, eval_df\n",
    "\n",
    "fiveg_train_df, _ = display_ratio(fiveg_df, 'FIVEG')\n",
    "other_train_df, _ = display_ratio(other_df, 'OTHER')\n",
    "nocon_train_df, _ = display_ratio(nocon_df, 'NOCON')\n",
    "\n",
    "df = pd.concat([fiveg_df, other_df, nocon_df])\n",
    "\n",
    "train_df, eval_df = display_ratio(df, 'TOTAL')\n",
    "\n",
    "X_train = train_df['full_text']\n",
    "y_train = train_df['label']\n",
    "\n",
    "X_eval = eval_df['full_text']\n",
    "y_eval = eval_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_test = False\n",
    "if no_test :\n",
    "    X_train = X_train.append(X_eval)\n",
    "    y_train = y_train.append(y_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# preprocessing #\n",
    "#################\n",
    "\n",
    "class Preprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.re_prog_url = re.compile(r'https://t.co/([a-zA-Z0-9]+)')\n",
    "    \n",
    "    def fit( self, X, y=None ):\n",
    "        return self \n",
    "    \n",
    "    def _process(self, text):\n",
    "        \n",
    "        urls = self.re_prog_url.findall(text)\n",
    "        text = text.lower()\\\n",
    "                .replace('https://t.co/', '')\\\n",
    "                .replace('u.s.', 'us')\\\n",
    "                .replace('u.k.', 'uk')\n",
    "        for url in urls:\n",
    "            text = text.replace(url.lower(), 'url')\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        X = X.apply(self._process)\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression\n",
      "\n",
      "Accuracy  Precision  Recall   F1       MCC\n",
      "74.16%    59.04%     53.50%   54.49%   42.38%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "############\n",
    "# pipeline #\n",
    "############\n",
    "\n",
    "classifier = LogisticRegression(\n",
    "    C=0.9,\n",
    "    class_weight={\n",
    "        1: 0.4,\n",
    "        2: 0.4,\n",
    "        3: 0.2\n",
    "    },\n",
    "    multi_class= 'ovr',\n",
    "    max_iter=2000,\n",
    "    solver= 'saga'\n",
    ")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    strip_accents='unicode',\n",
    ")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        ('preprocessor', Preprocessor()),\n",
    "        ('vectorizer', vectorizer),\n",
    "        ('classifier', classifier)\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_eval)\n",
    "probabilities = pipeline.predict_proba(X_eval)\n",
    "\n",
    "accuracy = accuracy_score(y_eval, predictions)*100\n",
    "precision = precision_score(y_eval, predictions, zero_division=0, average=\"macro\")*100\n",
    "recall = recall_score(y_eval, predictions, average=\"macro\")*100\n",
    "f1 = f1_score(y_eval, predictions, average=\"macro\")*100\n",
    "support = precision_recall_fscore_support(y_eval, predictions, average=\"macro\")\n",
    "matthews = matthews_corrcoef(y_eval, predictions)*100\n",
    "\n",
    "header = classifier.__class__.__name__\n",
    "\n",
    "print(f'\\n{header}\\n\\nAccuracy  Precision  Recall   F1       MCC')\n",
    "print(f'{accuracy:.2f}%{precision:>9.2f}%{recall:>10.2f}%{f1:>8.2f}%{matthews:>8.2f}%\\n')\n",
    "\n",
    "##############\n",
    "# submission #\n",
    "##############\n",
    "\n",
    "predictions = pipeline.predict(test_df['full_text'])\n",
    "probabilities = pipeline.predict_proba(test_df['full_text'])\n",
    "\n",
    "filename = './output/ME20FND_DL-TXST_001a.txt'\n",
    "if no_test:\n",
    "    filename = './output/ME20FND_DL-TXST_001b.txt'\n",
    "\n",
    "threshold = 0.10\n",
    "    \n",
    "with open(filename,'w') as f:\n",
    "    for tweet_id, prediction, prob in zip(test_df['id'], predictions, probabilities):\n",
    "        \n",
    "        diff = prob[prediction-1]-st.median(prob)\n",
    "        if diff < threshold:\n",
    "            prediction = 0\n",
    "        \n",
    "        f.write(f'{tweet_id},{prediction}\\n')\n",
    "    for tweet_id in missing_test_tweets:\n",
    "        f.write(f'{tweet_id},-1\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                                        #####################\n",
    "                                        # stop here for now #\n",
    "                                        #####################\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIVEG:\n",
      "\n",
      "boss\n",
      "victim\n",
      "fetched\n",
      "2009\n",
      "play\n",
      "60ghz\n",
      "provide\n",
      "humanity\n",
      "up\n",
      "shall\n",
      "activated\n",
      "warned\n",
      "follow\n",
      "areas\n",
      "chemicals\n",
      "popping\n",
      "zoonotic\n",
      "mention\n",
      "night\n",
      "looking\n",
      "globally\n",
      "power\n",
      "hemoglobin\n",
      "keep\n",
      "mandatory\n",
      "deprivation\n",
      "demolition\n",
      "cough\n",
      "spots\n",
      "controlled\n",
      "outs\n",
      "susceptible\n",
      "5gtowers\n",
      "surrounding\n",
      "function\n",
      "david\n",
      "disease\n",
      "harms\n",
      "chemtrails\n",
      "video\n",
      "lying\n",
      "asking\n",
      "investigating\n",
      "whistleblower\n",
      "worse\n",
      "map\n",
      "antennas\n",
      "out\n",
      "ain\n",
      "sheeple\n",
      "population\n",
      "destroy\n",
      "project\n",
      "cv\n",
      "structure\n",
      "5gkills\n",
      "distract\n",
      "radiation\n",
      "waves\n",
      "nwo\n",
      "weapon\n",
      "agenda\n",
      "realdonaldtrump\n",
      "distancing\n",
      "flu\n",
      "order\n",
      "question\n",
      "prove\n",
      "alter\n",
      "high\n",
      "everywhere\n",
      "21\n",
      "body\n",
      "knew\n",
      "chris\n",
      "connection\n",
      "place\n",
      "dying\n",
      "former\n",
      "billgates\n",
      "emf\n",
      "electromagnetic\n",
      "oxygen\n",
      "link\n",
      "tell\n",
      "coincidence\n",
      "illness\n",
      "research\n",
      "city\n",
      "microwave\n",
      "watch\n",
      "turned\n",
      "depopulation\n",
      "rolled\n",
      "5g\n",
      "kill\n",
      "share\n",
      "immune\n",
      "wuhan\n",
      "symptoms\n",
      "\n",
      "OTHER:\n",
      "\n",
      "sounds\n",
      "rfid\n",
      "totally\n",
      "assertions\n",
      "hysteria\n",
      "children\n",
      "but\n",
      "lets\n",
      "diversion\n",
      "usually\n",
      "chinese\n",
      "came\n",
      "22\n",
      "doctors\n",
      "real\n",
      "communication\n",
      "wake\n",
      "abuja\n",
      "manufactures\n",
      "shocking\n",
      "censoring\n",
      "down\n",
      "diagnosis\n",
      "weather\n",
      "2017\n",
      "health\n",
      "combating\n",
      "fuck\n",
      "say\n",
      "discussions\n",
      "chips\n",
      "worry\n",
      "while\n",
      "reason\n",
      "bill\n",
      "effects\n",
      "studies\n",
      "fake\n",
      "fauci\n",
      "safety\n",
      "implementation\n",
      "done\n",
      "invented\n",
      "american\n",
      "law\n",
      "funded\n",
      "term\n",
      "adverse\n",
      "warfare\n",
      "electro\n",
      "behind\n",
      "mn\n",
      "expert\n",
      "gmt\n",
      "played\n",
      "microchip\n",
      "rights\n",
      "shouldn\n",
      "pretext\n",
      "666\n",
      "adrenochrome\n",
      "locking\n",
      "held\n",
      "viruses\n",
      "ago\n",
      "nigerians\n",
      "likely\n",
      "camp\n",
      "felt\n",
      "safe\n",
      "msm\n",
      "stated\n",
      "war\n",
      "military\n",
      "communist\n",
      "put\n",
      "years\n",
      "ever\n",
      "every\n",
      "sickness\n",
      "familiar\n",
      "liz\n",
      "third\n",
      "africa\n",
      "criticize\n",
      "prayers\n",
      "control\n",
      "absurd\n",
      "thoughts\n",
      "should\n",
      "pushed\n",
      "evil\n",
      "schools\n",
      "bioweapon\n",
      "humans\n",
      "bad\n",
      "stop5g\n",
      "cancer\n",
      "lab\n",
      "installed\n",
      "\n",
      "NOCON:\n",
      "\n",
      "rays\n",
      "impact\n",
      "ridiculous\n",
      "spectrum\n",
      "statement\n",
      "total\n",
      "national\n",
      "gives\n",
      "community\n",
      "use\n",
      "baseless\n",
      "countries\n",
      "sent\n",
      "people\n",
      "causing\n",
      "birmingham\n",
      "burned\n",
      "black\n",
      "iran\n",
      "argument\n",
      "communications\n",
      "generation\n",
      "professor\n",
      "transmitted\n",
      "ll\n",
      "explain\n",
      "cases\n",
      "solutions\n",
      "lead\n",
      "literally\n",
      "telecommunications\n",
      "masts\n",
      "hospital\n",
      "come\n",
      "killed\n",
      "how\n",
      "security\n",
      "since\n",
      "nhs\n",
      "bro\n",
      "for\n",
      "nobody\n",
      "telling\n",
      "iphone\n",
      "linking\n",
      "mean\n",
      "broadband\n",
      "america\n",
      "thinking\n",
      "folks\n",
      "healthcare\n",
      "infrastructure\n",
      "hard\n",
      "works\n",
      "early\n",
      "spreads\n",
      "better\n",
      "think\n",
      "thanks\n",
      "ai\n",
      "her\n",
      "hope\n",
      "hoax\n",
      "business\n",
      "pray\n",
      "day\n",
      "spreading\n",
      "because\n",
      "company\n",
      "claim\n",
      "networks\n",
      "engineers\n",
      "antichrist\n",
      "nonsense\n",
      "theorists\n",
      "means\n",
      "south\n",
      "conspiracy\n",
      "viral\n",
      "spread\n",
      "fire\n",
      "year\n",
      "social\n",
      "britain\n",
      "stupid\n",
      "hear\n",
      "claiming\n",
      "flat\n",
      "next\n",
      "mobile\n",
      "huawei\n",
      "idea\n",
      "whatsapp\n",
      "believes\n",
      "conspiracies\n",
      "facebook\n",
      "misinformation\n",
      "thinks\n",
      "crisis\n",
      "burning\n",
      "\n"
     ]
    }
   ],
   "source": [
    "m = {1:'FIVEG',2:'OTHER',3:'NOCON'}\n",
    "\n",
    "def print_top100(vectorizer, clf, class_labels):    \n",
    "    \"\"\"Prints features with the highest coefficient values, per class\"\"\"\n",
    "    \n",
    "    feature_names = vectorizer.get_feature_names()\n",
    "    \n",
    "    for i, class_label in enumerate(class_labels):\n",
    "        top100 = np.argsort(clf.coef_[i])[-100:]\n",
    "        print(\"%s:\\n\\n%s\\n\" % (m[class_label],\n",
    "              \"\\n\".join(feature_names[j] for j in top100)))\n",
    "        \n",
    "print_top100(vectorizer, classifier, [1,2,3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
